<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Object Detection for Dummies Part 2: CNN, DPM and Overfeat | Lil&#39;Log</title>
<meta name="keywords" content="object-detection, object-recognition, vision-model" />
<meta name="description" content="Part 1 of the &ldquo;Object Detection for Dummies&rdquo; series introduced: (1) the concept of image gradient vector and how HOG algorithm summarizes the information across all the gradient vectors in one image; (2) how the image segmentation algorithm works to detect regions that potentially contain objects; (3) how the Selective Search algorithm refines the outcomes of image segmentation for better region proposal.
In Part 2, we are about to find out more on the classic convolution neural network architectures for image classification.">
<meta name="author" content="Lilian Weng">
<link rel="canonical" href="https://wuxb09.github.io/test-lilian/posts/2017-12-15-object-recognition-part-2/" />
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.67a6fb6e33089cb29e856bcc95d7aa39f70049a42b123105531265a0d9f1258b.css" integrity="sha256-Z6b7bjMInLKehWvMldeqOfcASaQrEjEFUxJloNnxJYs=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js" integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5&#43;kdJvBz5iKbt6B5PJI="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://wuxb09.github.io/test-lilian/favicon_peach.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://wuxb09.github.io/test-lilian/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://wuxb09.github.io/test-lilian/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://wuxb09.github.io/test-lilian/apple-touch-icon.png">
<link rel="mask-icon" href="https://wuxb09.github.io/test-lilian/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-HFT45VFBX6"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-HFT45VFBX6', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="Object Detection for Dummies Part 2: CNN, DPM and Overfeat" />
<meta property="og:description" content="Part 1 of the &ldquo;Object Detection for Dummies&rdquo; series introduced: (1) the concept of image gradient vector and how HOG algorithm summarizes the information across all the gradient vectors in one image; (2) how the image segmentation algorithm works to detect regions that potentially contain objects; (3) how the Selective Search algorithm refines the outcomes of image segmentation for better region proposal.
In Part 2, we are about to find out more on the classic convolution neural network architectures for image classification." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://wuxb09.github.io/test-lilian/posts/2017-12-15-object-recognition-part-2/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2017-12-15T00:00:00&#43;00:00" />
<meta property="article:modified_time" content="2017-12-15T00:00:00&#43;00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Object Detection for Dummies Part 2: CNN, DPM and Overfeat"/>
<meta name="twitter:description" content="Part 1 of the &ldquo;Object Detection for Dummies&rdquo; series introduced: (1) the concept of image gradient vector and how HOG algorithm summarizes the information across all the gradient vectors in one image; (2) how the image segmentation algorithm works to detect regions that potentially contain objects; (3) how the Selective Search algorithm refines the outcomes of image segmentation for better region proposal.
In Part 2, we are about to find out more on the classic convolution neural network architectures for image classification."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://wuxb09.github.io/test-lilian/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Object Detection for Dummies Part 2: CNN, DPM and Overfeat",
      "item": "https://wuxb09.github.io/test-lilian/posts/2017-12-15-object-recognition-part-2/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Object Detection for Dummies Part 2: CNN, DPM and Overfeat",
  "name": "Object Detection for Dummies Part 2: CNN, DPM and Overfeat",
  "description": "Part 1 of the \u0026ldquo;Object Detection for Dummies\u0026rdquo; series introduced: (1) the concept of image gradient vector and how HOG algorithm summarizes the information across all the gradient vectors in one image; (2) how the image segmentation algorithm works to detect regions that potentially contain objects; (3) how the Selective Search algorithm refines the outcomes of image segmentation for better region proposal.\nIn Part 2, we are about to find out more on the classic convolution neural network architectures for image classification.",
  "keywords": [
    "object-detection", "object-recognition", "vision-model"
  ],
  "articleBody": "Part 1 of the “Object Detection for Dummies” series introduced: (1) the concept of image gradient vector and how HOG algorithm summarizes the information across all the gradient vectors in one image; (2) how the image segmentation algorithm works to detect regions that potentially contain objects; (3) how the Selective Search algorithm refines the outcomes of image segmentation for better region proposal.\nIn Part 2, we are about to find out more on the classic convolution neural network architectures for image classification. They lay the foundation for further progress on the deep learning models for object detection. Go check Part 3 if you want to learn more on R-CNN and related models.\nLinks to all the posts in the series: [Part 1] [Part 2] [Part 3] [Part 4].\nCNN for Image Classification CNN, short for “Convolutional Neural Network”, is the go-to solution for computer vision problems in the deep learning world. It was, to some extent, inspired by how human visual cortex system works.\nConvolution Operation I strongly recommend this guide to convolution arithmetic, which provides a clean and solid explanation with tons of visualizations and examples. Here let’s focus on two-dimensional convolution as we are working with images in this post.\nIn short, convolution operation slides a predefined kernel (also called “filter”) on top of the input feature map (matrix of image pixels), multiplying and adding the values of the kernel and partial input features to generate the output. The values form an output matrix, as usually, the kernel is much smaller than the input image.\nFig. 1. An illustration of applying a kernel on the input feature map to generate the output. (Image source: River Trail documentation) Figure 2 showcases two real examples of how to convolve a 3x3 kernel over a 5x5 2D matrix of numeric values to generate a 3x3 matrix. By controlling the padding size and the stride length, we can generate an output matrix of a certain size.\nFig. 2. Two examples of 2D convolution operation: (top) no padding and 1x1 strides; (bottom) 1x1 border zeros padding and 2x2 strides. (Image source: deeplearning.net) AlexNet (Krizhevsky et al, 2012)  5 convolution [+ optional max pooling] layers + 2 MLP layers + 1 LR layer Use data augmentation techniques to expand the training dataset, such as image translations, horizontal reflections, and patch extractions.  Fig. 3. The architecture of AlexNet. (Image source: link) VGG (Simonyan and Zisserman, 2014)  The network is considered as “very deep” at its time; 19 layers The architecture is extremely simplified with only 3x3 convolutional layers and 2x2 pooling layers. The stacking of small filters simulates a larger filter with fewer parameters.  ResNet (He et al., 2015)  The network is indeed very deep; 152 layers of simple architecture. Residual Block: Some input of a certain layer can be passed to the component two layers later. Residual blocks are essential for keeping a deep network trainable and eventually work. Without residual blocks, the training loss of a plain network does not monotonically decrease as the number of layers increases due to vanishing and exploding gradients.  Fig. 4. An illustration of the residual block of ResNet. In some way, we can say the design of residual blocks is inspired by V4 getting input directly from V1 in the human visual cortex system. (left image source: Wang et al., 2017) Evaluation Metrics: mAP A common evaluation metric used in many object recognition and detection tasks is “mAP”, short for “mean average precision”. It is a number from 0 to 100; higher value is better.\n Combine all detections from all test images to draw a precision-recall curve (PR curve) for each class; The “average precision” (AP) is the area under the PR curve. Given that target objects are in different classes, we first compute AP separately for each class, and then average over classes. A detection is a true positive if it has “intersection over union” (IoU) with a ground-truth box greater than some threshold (usually 0.5; if so, the metric is “mAP@0.5”)  Deformable Parts Model The Deformable Parts Model (DPM) (Felzenszwalb et al., 2010) recognizes objects with a mixture graphical model (Markov random fields) of deformable parts. The model consists of three major components:\n A coarse root filter defines a detection window that approximately covers an entire object. A filter specifies weights for a region feature vector. Multiple part filters that cover smaller parts of the object. Parts filters are learned at twice resolution of the root filter. A spatial model for scoring the locations of part filters relative to the root.  Fig. 5. The DPM model contains (a) a root filter, (b) multiple part filters at twice the resolution, and (c) a model for scoring the location and deformation of parts. The quality of detecting an object is measured by the score of filters minus the deformation costs. The matching score $f$, in laymen’s terms, is:\n $$ f(\\text{model}, x) = f(\\beta_\\text{root}, x) + \\sum_{\\beta_\\text{part} \\in \\text{part filters}} \\max_y [f(\\beta_\\text{part}, y) - \\text{cost}(\\beta_\\text{part}, x, y)] $$  in which,\n $x$ is an image with a specified position and scale; $y$ is a sub region of $x$. $\\beta_\\text{root}$ is the root filter. $\\beta_\\text{part}$ is one part filter. cost() measures the penalty of the part deviating from its ideal location relative to the root.  The basic score model is the dot product between the filter $\\beta$ and the region feature vector $\\Phi(x)$: $f(\\beta, x) = \\beta \\cdot \\Phi(x)$. The feature set $\\Phi(x)$ can be defined by HOG or other similar algorithms.\nA root location with high score detects a region with high chances to contain an object, while the locations of the parts with high scores confirm a recognized object hypothesis. The paper adopted latent SVM to model the classifier.\nFig. 6. The matching process by DPM. (Image source: Felzenszwalb et al., 2010) The author later claimed that DPM and CNN models are not two distinct approaches to object recognition. Instead, a DPM model can be formulated as a CNN by unrolling the DPM inference algorithm and mapping each step to an equivalent CNN layer. (Check the details in Girshick et al., 2015!)\nOverfeat Overfeat [paper][code] is a pioneer model of integrating the object detection, localization and classification tasks all into one convolutional neural network. The main idea is to (i) do image classification at different locations on regions of multiple scales of the image in a sliding window fashion, and (ii) predict the bounding box locations with a regressor trained on top of the same convolution layers.\nThe Overfeat model architecture is very similar to AlexNet. It is trained as follows:\nFig. 7. The training stages of the Overfeat model. (Image source: link)  Train a CNN model (similar to AlexNet) on the image classification task. Then, we replace the top classifier layers by a regression network and train it to predict object bounding boxes at each spatial location and scale. The regressor is class-specific, each generated for one image class.  Input: Images with classification and bounding box. Output: $(x_\\text{left}, x_\\text{right}, y_\\text{top}, y_\\text{bottom})$, 4 values in total, representing the coordinates of the bounding box edges. Loss: The regressor is trained to minimize $l2$ norm between generated bounding box and the ground truth for each training example.    At the detection time,\n Perform classification at each location using the pretrained CNN model. Predict object bounding boxes on all classified regions generated by the classifier. Merge bounding boxes with sufficient overlap from localization and sufficient confidence of being the same object from the classifier.   Cited as:\n@article{weng2017detection2, title = \"Object Detection for Dummies Part 2: CNN, DPM and Overfeat\", author = \"Weng, Lilian\", journal = \"wuxb09.github.io/test-lilian\", year = \"2017\", url = \"https://wuxb09.github.io/test-lilian/posts/2017-12-15-object-recognition-part-2/\" } Reference [1] Vincent Dumoulin and Francesco Visin. “A guide to convolution arithmetic for deep learning.\" arXiv preprint arXiv:1603.07285 (2016).\n[2] Haohan Wang, Bhiksha Raj, and Eric P. Xing. “On the Origin of Deep Learning.\" arXiv preprint arXiv:1702.07800 (2017).\n[3] Pedro F. Felzenszwalb, Ross B. Girshick, David McAllester, and Deva Ramanan. “Object detection with discriminatively trained part-based models.\" IEEE transactions on pattern analysis and machine intelligence 32, no. 9 (2010): 1627-1645.\n[4] Ross B. Girshick, Forrest Iandola, Trevor Darrell, and Jitendra Malik. “Deformable part models are convolutional neural networks.\" In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), pp. 437-446. 2015.\n[5] Sermanet, Pierre, David Eigen, Xiang Zhang, Michaël Mathieu, Rob Fergus, and Yann LeCun. “OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks” arXiv preprint arXiv:1312.6229 (2013).\n",
  "wordCount" : "1400",
  "inLanguage": "en",
  "datePublished": "2017-12-15T00:00:00Z",
  "dateModified": "2017-12-15T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Lilian Weng"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://wuxb09.github.io/test-lilian/posts/2017-12-15-object-recognition-part-2/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Lil'Log",
    "logo": {
      "@type": "ImageObject",
      "url": "https://wuxb09.github.io/test-lilian/favicon_peach.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://wuxb09.github.io/test-lilian/" accesskey="h" title="Lil&#39;Log (Alt + H)">Lil&#39;Log</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="https://wuxb09.github.io/test-lilian/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://wuxb09.github.io/test-lilian/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://wuxb09.github.io/test-lilian/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://wuxb09.github.io/test-lilian/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://wuxb09.github.io/test-lilian/faq" title="FAQ">
                    <span>FAQ</span>
                </a>
            </li>
            <li>
                <a href="https://www.emojisearch.app/" title="emojisearch.app">
                    <span>emojisearch.app</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      Object Detection for Dummies Part 2: CNN, DPM and Overfeat
    </h1>
    <div class="post-meta"><span title='2017-12-15 00:00:00 +0000 UTC'>December 15, 2017</span>&nbsp;·&nbsp;7 min&nbsp;·&nbsp;Lilian Weng

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#cnn-for-image-classification" aria-label="CNN for Image Classification">CNN for Image Classification</a><ul>
                        
                <li>
                    <a href="#convolution-operation" aria-label="Convolution Operation">Convolution Operation</a></li>
                <li>
                    <a href="#alexnet-krizhevsky-et-al-2012" aria-label="AlexNet (Krizhevsky et al, 2012)">AlexNet (Krizhevsky et al, 2012)</a></li>
                <li>
                    <a href="#vgg-simonyan-and-zisserman-2014" aria-label="VGG (Simonyan and Zisserman, 2014)">VGG (Simonyan and Zisserman, 2014)</a></li>
                <li>
                    <a href="#resnet-he-et-al-2015" aria-label="ResNet (He et al., 2015)">ResNet (He et al., 2015)</a></li></ul>
                </li>
                <li>
                    <a href="#evaluation-metrics-map" aria-label="Evaluation Metrics: mAP">Evaluation Metrics: mAP</a></li>
                <li>
                    <a href="#deformable-parts-model" aria-label="Deformable Parts Model">Deformable Parts Model</a></li>
                <li>
                    <a href="#overfeat" aria-label="Overfeat">Overfeat</a></li>
                <li>
                    <a href="#reference" aria-label="Reference">Reference</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><!-- Part 2 introduces several classic convolutional neural work architecture designs for image classification (AlexNet, VGG, ResNet), as well as DPM (Deformable Parts Model) and Overfeat models for object recognition. -->
<p><a href="https://wuxb09.github.io/test-lilian/posts/2017-10-29-object-recognition-part-1/">Part 1</a> of the &ldquo;Object Detection for Dummies&rdquo; series introduced: (1) the concept of image gradient vector and how HOG algorithm summarizes the information across all the gradient vectors in one image; (2) how the image segmentation algorithm works to detect regions that potentially contain objects; (3) how the Selective Search algorithm refines the outcomes of image segmentation for better region proposal.</p>
<p>In Part 2, we are about to find out more on the classic convolution neural network architectures for image classification. They lay the <em><strong>foundation</strong></em> for further progress on the deep learning models for object detection. Go check <a href="https://wuxb09.github.io/test-lilian/posts/2017-12-31-object-recognition-part-3/">Part 3</a> if you want to learn more on R-CNN and related models.</p>
<p>Links to all the posts in the series:
[<a href="https://wuxb09.github.io/test-lilian/posts/2017-10-29-object-recognition-part-1/">Part 1</a>]
[<a href="https://wuxb09.github.io/test-lilian/posts/2017-12-15-object-recognition-part-2/">Part 2</a>]
[<a href="https://wuxb09.github.io/test-lilian/posts/2017-12-31-object-recognition-part-3/">Part 3</a>]
[<a href="https://wuxb09.github.io/test-lilian/posts/2018-12-27-object-recognition-part-4/">Part 4</a>].</p>
<h1 id="cnn-for-image-classification">CNN for Image Classification<a hidden class="anchor" aria-hidden="true" href="#cnn-for-image-classification">#</a></h1>
<p>CNN, short for &ldquo;<strong>Convolutional Neural Network</strong>&rdquo;, is the go-to solution for computer vision problems in the deep learning world. It was, to some extent, <a href="https://wuxb09.github.io/test-lilian/posts/2017-06-21-overview/#convolutional-neural-network">inspired</a> by how human visual cortex system works.</p>
<h2 id="convolution-operation">Convolution Operation<a hidden class="anchor" aria-hidden="true" href="#convolution-operation">#</a></h2>
<p>I strongly recommend this <a href="https://arxiv.org/pdf/1603.07285.pdf">guide</a> to convolution arithmetic, which provides a clean and solid explanation with tons of visualizations and examples. Here let&rsquo;s focus on two-dimensional convolution as we are working with images in this post.</p>
<p>In short, convolution operation slides a predefined <a href="https://en.wikipedia.org/wiki/Kernel_(image_processing)">kernel</a> (also called &ldquo;filter&rdquo;) on top of the input feature map (matrix of image pixels), multiplying and adding the values of the kernel and partial input features to generate the output. The values form an output matrix, as usually, the kernel is much smaller than the input image.</p>
<img src="convolution-operation.png" style="width: 500px;" class="center" />
<figcaption>Fig. 1. An illustration of applying a kernel on the input feature map to generate the output. (Image source: <a href="http://intellabs.github.io/RiverTrail/tutorial/" target="_blank">River Trail documentation</a>)</figcaption>
<p>Figure 2 showcases two real examples of how to convolve a 3x3 kernel over a 5x5 2D matrix of numeric values to generate a 3x3 matrix. By controlling the padding size and the stride length, we can generate an output matrix of a certain size.</p>
<img src="numerical_no_padding_no_strides.gif" class="center" />
<img src="numerical_padding_strides.gif" class="center" />
<figcaption>Fig. 2. Two examples of 2D convolution operation: (top) no padding and 1x1 strides; (bottom) 1x1 border zeros padding and 2x2 strides. (Image source: <a href="http://deeplearning.net/software/theano_versions/dev/tutorial/conv_arithmetic.html" target="_blank">deeplearning.net</a>)</figcaption>
<h2 id="alexnet-krizhevsky-et-al-2012">AlexNet (Krizhevsky et al, 2012)<a hidden class="anchor" aria-hidden="true" href="#alexnet-krizhevsky-et-al-2012">#</a></h2>
<ul>
<li>5 convolution [+ optional max pooling] layers + 2 MLP layers + 1 LR layer</li>
<li>Use data augmentation techniques to expand the training dataset, such as image translations, horizontal reflections, and patch extractions.</li>
</ul>
<img src="alex_net_illustration.png" style="width: 100%;" class="center" />
<figcaption>Fig. 3. The architecture of AlexNet. (Image source: <a href="http://vision03.csail.mit.edu/cnn_art/index.html" target="_blank">link</a>)</figcaption>
<h2 id="vgg-simonyan-and-zisserman-2014">VGG (Simonyan and Zisserman, 2014)<a hidden class="anchor" aria-hidden="true" href="#vgg-simonyan-and-zisserman-2014">#</a></h2>
<ul>
<li>The network is considered as &ldquo;very deep&rdquo; at its time; 19 layers</li>
<li>The architecture is extremely simplified with only 3x3 convolutional layers and 2x2 pooling layers. The stacking of small filters simulates a larger filter with fewer parameters.</li>
</ul>
<h2 id="resnet-he-et-al-2015">ResNet (He et al., 2015)<a hidden class="anchor" aria-hidden="true" href="#resnet-he-et-al-2015">#</a></h2>
<ul>
<li>The network is indeed very deep; 152 layers of simple architecture.</li>
<li><strong>Residual Block</strong>: Some input of a certain layer can be passed to the component two layers later. Residual blocks are essential for keeping a deep network trainable and eventually work. Without residual blocks, the training loss of a plain network does not monotonically decrease as the number of layers increases due to <a href="http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/">vanishing and exploding gradients</a>.</li>
</ul>
<img src="residual-block.png" style="width: 100%;" class="center" />
<figcaption>Fig. 4. An illustration of the residual block of ResNet. In some way, we can say the design of residual blocks is inspired by V4 getting input directly from V1 in the human visual cortex system. (left image source: <a href="https://arxiv.org/pdf/1312.6229.pdf" target="_blank">Wang et al., 2017</a>)</figcaption>
<h1 id="evaluation-metrics-map">Evaluation Metrics: mAP<a hidden class="anchor" aria-hidden="true" href="#evaluation-metrics-map">#</a></h1>
<p>A common evaluation metric used in many object recognition and detection tasks is &ldquo;<strong>mAP</strong>&rdquo;, short for &ldquo;<strong>mean average precision</strong>&rdquo;. It is a number from 0 to 100; higher value is better.</p>
<ul>
<li>Combine all detections from all test images to draw a precision-recall curve (PR curve) for each class; The &ldquo;average precision&rdquo; (AP) is the area under the PR curve.</li>
<li>Given that target objects are in different classes, we first compute AP separately for each class, and then average over classes.</li>
<li>A detection is a true positive if it has <strong>&ldquo;intersection over union&rdquo; (IoU)</strong> with a ground-truth box greater than some threshold (usually 0.5; if so, the metric is &ldquo;<a href="mailto:mAP@0.5">mAP@0.5</a>&rdquo;)</li>
</ul>
<h1 id="deformable-parts-model">Deformable Parts Model<a hidden class="anchor" aria-hidden="true" href="#deformable-parts-model">#</a></h1>
<p>The Deformable Parts Model (DPM) (<a href="http://people.cs.uchicago.edu/~pff/papers/lsvm-pami.pdf">Felzenszwalb et al., 2010</a>) recognizes objects with a mixture graphical model (Markov random fields) of deformable parts. The model consists of three major components:</p>
<ol>
<li>A coarse <em><strong>root filter</strong></em> defines a detection window that approximately covers an entire object. A filter specifies weights for a region feature vector.</li>
<li>Multiple <em><strong>part filters</strong></em> that cover smaller parts of the object. Parts filters are learned at twice resolution of the root filter.</li>
<li>A <em><strong>spatial model</strong></em> for scoring the locations of part filters relative to the root.</li>
</ol>
<img src="DPM.png" style="width: 100%;" class="center" />
<figcaption>Fig. 5. The DPM model contains (a) a root filter, (b) multiple part filters at twice the resolution, and (c) a model for scoring the location and deformation of parts.</figcaption>
<p>The quality of detecting an object is measured by the score of filters minus the deformation costs. The matching score $f$, in laymen&rsquo;s terms, is:</p>
<div>
$$
f(\text{model}, x) = f(\beta_\text{root}, x) + \sum_{\beta_\text{part} \in \text{part filters}} \max_y [f(\beta_\text{part}, y) - \text{cost}(\beta_\text{part}, x, y)]
$$
</div>
<p>in which,</p>
<ul>
<li>$x$ is an image with a specified position and scale;</li>
<li>$y$ is a sub region of $x$.</li>
<li>$\beta_\text{root}$ is the root filter.</li>
<li>$\beta_\text{part}$ is one part filter.</li>
<li>cost() measures the penalty of the part deviating from its ideal location relative to the root.</li>
</ul>
<p>The basic score model is the dot product between the filter $\beta$ and the region feature vector $\Phi(x)$: $f(\beta, x) = \beta \cdot \Phi(x)$. The feature set $\Phi(x)$ can be defined by HOG or other similar algorithms.</p>
<p>A root location with high score detects a region with high chances to contain an object, while the locations of the parts with high scores confirm a recognized object hypothesis. The paper adopted latent SVM to model the classifier.</p>
<img src="DPM-matching.png" style="width: 100%;" class="center" />
<figcaption>Fig. 6. The matching process by DPM. (Image source: <a href="http://people.cs.uchicago.edu/~pff/papers/lsvm-pami.pdf" target="_blank">Felzenszwalb et al., 2010</a>)</figcaption>
<p>The author later claimed that DPM and CNN models are not two distinct approaches to object recognition. Instead, a DPM model can be formulated as a CNN by unrolling the DPM inference algorithm and mapping each step to an equivalent CNN layer. (Check the details in <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Girshick_Deformable_Part_Models_2015_CVPR_paper.pdf">Girshick et al., 2015</a>!)</p>
<h1 id="overfeat">Overfeat<a hidden class="anchor" aria-hidden="true" href="#overfeat">#</a></h1>
<p>Overfeat [<a href="https://pdfs.semanticscholar.org/f2c2/fbc35d0541571f54790851de9fcd1adde085.pdf">paper</a>][<a href="https://github.com/sermanet/OverFeat">code</a>] is a pioneer model of integrating the object detection, localization and classification tasks all into one convolutional neural network. The main idea is to (i) do image classification at different locations on regions of multiple scales of the image in a sliding window fashion, and (ii) predict the bounding box locations with a regressor trained on top of the same convolution layers.</p>
<p>The Overfeat model architecture is very similar to <a href="#alexnet-krizhevsky-et-al-2012">AlexNet</a>. It is trained as follows:</p>
<img src="overfeat-training.png" style="width: 400px;" class="center" />
<figcaption>Fig. 7. The training stages of the Overfeat model. (Image source: <a href="http://vision.stanford.edu/teaching/cs231b_spring1415/slides/overfeat_eric.pdf" target="_blank">link</a>)</figcaption>
<ol>
<li>Train a CNN model (similar to AlexNet) on the image classification task.</li>
<li>Then, we replace the top classifier layers by a regression network and train it to predict object bounding boxes at each spatial location and scale. The regressor is class-specific, each generated for one image class.
<ul>
<li>Input: Images with classification and bounding box.</li>
<li>Output: $(x_\text{left}, x_\text{right}, y_\text{top}, y_\text{bottom})$, 4 values in total, representing the coordinates of the bounding box edges.</li>
<li>Loss: The regressor is trained to minimize $l2$ norm between generated bounding box and the ground truth for each training example.</li>
</ul>
</li>
</ol>
<p>At the detection time,</p>
<ol>
<li>Perform classification at each location using the pretrained CNN model.</li>
<li>Predict object bounding boxes on all classified regions generated by the classifier.</li>
<li>Merge bounding boxes with sufficient overlap from localization and sufficient confidence of being the same object from the classifier.</li>
</ol>
<hr>
<p>Cited as:</p>
<pre tabindex="0"><code>@article{weng2017detection2,
  title   = &quot;Object Detection for Dummies Part 2: CNN, DPM and Overfeat&quot;,
  author  = &quot;Weng, Lilian&quot;,
  journal = &quot;wuxb09.github.io/test-lilian&quot;,
  year    = &quot;2017&quot;,
  url     = &quot;https://wuxb09.github.io/test-lilian/posts/2017-12-15-object-recognition-part-2/&quot;
}
</code></pre><h1 id="reference">Reference<a hidden class="anchor" aria-hidden="true" href="#reference">#</a></h1>
<p>[1] Vincent Dumoulin and Francesco Visin. <a href="https://arxiv.org/pdf/1603.07285.pdf">&ldquo;A guide to convolution arithmetic for deep learning.&quot;</a> arXiv preprint arXiv:1603.07285 (2016).</p>
<p>[2] Haohan Wang, Bhiksha Raj, and Eric P. Xing. <a href="https://arxiv.org/pdf/1702.07800.pdf">&ldquo;On the Origin of Deep Learning.&quot;</a> arXiv preprint arXiv:1702.07800 (2017).</p>
<p>[3] Pedro F. Felzenszwalb, Ross B. Girshick, David McAllester, and Deva Ramanan. <a href="http://people.cs.uchicago.edu/~pff/papers/lsvm-pami.pdf">&ldquo;Object detection with discriminatively trained part-based models.&quot;</a> IEEE transactions on pattern analysis and machine intelligence 32, no. 9 (2010): 1627-1645.</p>
<p>[4] Ross B. Girshick, Forrest Iandola, Trevor Darrell, and Jitendra Malik. <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Girshick_Deformable_Part_Models_2015_CVPR_paper.pdf">&ldquo;Deformable part models are convolutional neural networks.&quot;</a> In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), pp. 437-446. 2015.</p>
<p>[5] Sermanet, Pierre, David Eigen, Xiang Zhang, Michaël Mathieu, Rob Fergus, and Yann LeCun. <a href="https://pdfs.semanticscholar.org/f2c2/fbc35d0541571f54790851de9fcd1adde085.pdf">&ldquo;OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks&rdquo;</a> arXiv preprint arXiv:1312.6229 (2013).</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://wuxb09.github.io/test-lilian/tags/object-detection/">object-detection</a></li>
      <li><a href="https://wuxb09.github.io/test-lilian/tags/object-recognition/">object-recognition</a></li>
      <li><a href="https://wuxb09.github.io/test-lilian/tags/vision-model/">vision-model</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://wuxb09.github.io/test-lilian/posts/2017-12-31-object-recognition-part-3/">
    <span class="title">« </span>
    <br>
    <span>Object Detection for Dummies Part 3: R-CNN Family</span>
  </a>
  <a class="next" href="https://wuxb09.github.io/test-lilian/posts/2017-10-29-object-recognition-part-1/">
    <span class="title"> »</span>
    <br>
    <span>Object Detection for Dummies Part 1: Gradient Vector, HOG, and SS</span>
  </a>
</nav>


<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share Object Detection for Dummies Part 2: CNN, DPM and Overfeat on twitter"
        href="https://twitter.com/intent/tweet/?text=Object%20Detection%20for%20Dummies%20Part%202%3a%20CNN%2c%20DPM%20and%20Overfeat&amp;url=https%3a%2f%2fwuxb09.github.io/test-lilian%2fposts%2f2017-12-15-object-recognition-part-2%2f&amp;hashtags=object-detection%2cobject-recognition%2cvision-model">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Object Detection for Dummies Part 2: CNN, DPM and Overfeat on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fwuxb09.github.io/test-lilian%2fposts%2f2017-12-15-object-recognition-part-2%2f&amp;title=Object%20Detection%20for%20Dummies%20Part%202%3a%20CNN%2c%20DPM%20and%20Overfeat&amp;summary=Object%20Detection%20for%20Dummies%20Part%202%3a%20CNN%2c%20DPM%20and%20Overfeat&amp;source=https%3a%2f%2fwuxb09.github.io/test-lilian%2fposts%2f2017-12-15-object-recognition-part-2%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Object Detection for Dummies Part 2: CNN, DPM and Overfeat on reddit"
        href="https://reddit.com/submit?url=https%3a%2f%2fwuxb09.github.io/test-lilian%2fposts%2f2017-12-15-object-recognition-part-2%2f&title=Object%20Detection%20for%20Dummies%20Part%202%3a%20CNN%2c%20DPM%20and%20Overfeat">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Object Detection for Dummies Part 2: CNN, DPM and Overfeat on facebook"
        href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fwuxb09.github.io/test-lilian%2fposts%2f2017-12-15-object-recognition-part-2%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Object Detection for Dummies Part 2: CNN, DPM and Overfeat on whatsapp"
        href="https://api.whatsapp.com/send?text=Object%20Detection%20for%20Dummies%20Part%202%3a%20CNN%2c%20DPM%20and%20Overfeat%20-%20https%3a%2f%2fwuxb09.github.io/test-lilian%2fposts%2f2017-12-15-object-recognition-part-2%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Object Detection for Dummies Part 2: CNN, DPM and Overfeat on telegram"
        href="https://telegram.me/share/url?text=Object%20Detection%20for%20Dummies%20Part%202%3a%20CNN%2c%20DPM%20and%20Overfeat&amp;url=https%3a%2f%2fwuxb09.github.io/test-lilian%2fposts%2f2017-12-15-object-recognition-part-2%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023 <a href="https://wuxb09.github.io/test-lilian/">Lil&#39;Log</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'copy';

        function copyingDone() {
            copybutton.innerText = 'copied!';
            setTimeout(() => {
                copybutton.innerText = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
