<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>safety on Lil&#39;Log</title>
    <link>https://wuxb09.github.io/test-lilian/tags/safety/</link>
    <description>Recent content in safety on Lil&#39;Log</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 21 Mar 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://wuxb09.github.io/test-lilian/tags/safety/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Reducing Toxicity in Language Models</title>
      <link>https://wuxb09.github.io/test-lilian/posts/2021-03-21-lm-toxicity/</link>
      <pubDate>Sun, 21 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>https://wuxb09.github.io/test-lilian/posts/2021-03-21-lm-toxicity/</guid>
      <description>Large pretrained language models are trained over a sizable collection of online data. They unavoidably acquire certain toxic behavior and biases from the Internet. Pretrained language models are very powerful and have shown great success in many NLP tasks. However, to safely deploy them for practical real-world applications demands a strong safety control over the model generation process.
Many challenges are associated with the effort to diminish various types of unsafe content:</description>
    </item>
    
  </channel>
</rss>
