<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>language-model on Lil&#39;Log</title>
    <link>https://wuxb09.github.io/test-lilian/tags/language-model/</link>
    <description>Recent content in language-model on Lil&#39;Log</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 23 Jun 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://wuxb09.github.io/test-lilian/tags/language-model/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>LLM Powered Autonomous Agents</title>
      <link>https://wuxb09.github.io/test-lilian/posts/2023-06-23-agent/</link>
      <pubDate>Fri, 23 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://wuxb09.github.io/test-lilian/posts/2023-06-23-agent/</guid>
      <description>Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.
Agent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent&amp;rsquo;s brain, complemented by several key components:</description>
    </item>
    
    <item>
      <title>Prompt Engineering</title>
      <link>https://wuxb09.github.io/test-lilian/posts/2023-03-15-prompt-engineering/</link>
      <pubDate>Wed, 15 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>https://wuxb09.github.io/test-lilian/posts/2023-03-15-prompt-engineering/</guid>
      <description>Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.
This post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models.</description>
    </item>
    
    <item>
      <title>Large Transformer Model Inference Optimization</title>
      <link>https://wuxb09.github.io/test-lilian/posts/2023-01-10-inference-optimization/</link>
      <pubDate>Tue, 10 Jan 2023 10:00:00 -0700</pubDate>
      
      <guid>https://wuxb09.github.io/test-lilian/posts/2023-01-10-inference-optimization/</guid>
      <description>[Updated on 2023-01-24: add a small section on Distillation.]
Large transformer models are mainstream nowadays, creating SoTA results for a variety of tasks. They are powerful but very expensive to train and use. The extremely high inference cost, in both time and memory, is a big bottleneck for adopting a powerful transformer for solving real-world tasks at scale.
Why is it hard to run inference for large transformer models? Besides the increasing size of SoTA models, there are two main factors contributing to the inference challenge (Pope et al.</description>
    </item>
    
    <item>
      <title>Generalized Visual Language Models</title>
      <link>https://wuxb09.github.io/test-lilian/posts/2022-06-09-vlm/</link>
      <pubDate>Thu, 09 Jun 2022 15:10:30 -0700</pubDate>
      
      <guid>https://wuxb09.github.io/test-lilian/posts/2022-06-09-vlm/</guid>
      <description>Processing images to generate text, such as image captioning and visual question-answering, has been studied for years. Traditionally such systems rely on an object detection network as a vision encoder to capture visual features and then produce text via a text decoder. Given a large amount of existing literature, in this post, I would like to only focus on one approach for solving vision language tasks, which is to extend pre-trained generalized language models to be capable of consuming visual signals.</description>
    </item>
    
    <item>
      <title>Contrastive Representation Learning</title>
      <link>https://wuxb09.github.io/test-lilian/posts/2021-05-31-contrastive/</link>
      <pubDate>Mon, 31 May 2021 00:00:00 +0000</pubDate>
      
      <guid>https://wuxb09.github.io/test-lilian/posts/2021-05-31-contrastive/</guid>
      <description>The goal of contrastive representation learning is to learn such an embedding space in which similar sample pairs stay close to each other while dissimilar ones are far apart. Contrastive learning can be applied to both supervised and unsupervised settings. When working with unsupervised data, contrastive learning is one of the most powerful approaches in self-supervised learning.
Contrastive Training Objectives In early versions of loss functions for contrastive learning, only one positive and one negative sample are involved.</description>
    </item>
    
    <item>
      <title>Reducing Toxicity in Language Models</title>
      <link>https://wuxb09.github.io/test-lilian/posts/2021-03-21-lm-toxicity/</link>
      <pubDate>Sun, 21 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>https://wuxb09.github.io/test-lilian/posts/2021-03-21-lm-toxicity/</guid>
      <description>Large pretrained language models are trained over a sizable collection of online data. They unavoidably acquire certain toxic behavior and biases from the Internet. Pretrained language models are very powerful and have shown great success in many NLP tasks. However, to safely deploy them for practical real-world applications demands a strong safety control over the model generation process.
Many challenges are associated with the effort to diminish various types of unsafe content:</description>
    </item>
    
    <item>
      <title>Controllable Neural Text Generation</title>
      <link>https://wuxb09.github.io/test-lilian/posts/2021-01-02-controllable-text-generation/</link>
      <pubDate>Sat, 02 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>https://wuxb09.github.io/test-lilian/posts/2021-01-02-controllable-text-generation/</guid>
      <description>[Updated on 2021-02-01: Updated to version 2.0 with several work added and many typos fixed.] [Updated on 2021-05-26: Add P-tuning and Prompt Tuning in the &amp;ldquo;prompt design&amp;rdquo; section.] [Updated on 2021-09-19: Add &amp;ldquo;unlikelihood training&amp;rdquo;.]
There is a gigantic amount of free text on the Web, several magnitude more than labelled benchmark datasets. The state-of-the-art language models (LM) are trained with unsupervised Web data in large scale. When generating samples from LM by iteratively sampling the next token, we do not have much control over attributes of the output text, such as the topic, the style, the sentiment, etc.</description>
    </item>
    
    <item>
      <title>How to Build an Open-Domain Question Answering System?</title>
      <link>https://wuxb09.github.io/test-lilian/posts/2020-10-29-odqa/</link>
      <pubDate>Thu, 29 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://wuxb09.github.io/test-lilian/posts/2020-10-29-odqa/</guid>
      <description>[Updated on 2020-11-12: add an example on closed-book factual QA using OpenAI API (beta).
A model that can answer any question with regard to factual knowledge can lead to many useful and practical applications, such as working as a chatbot or an AI assistantðŸ¤–. In this post, we will review several common approaches for building such an open-domain question answering system.
Disclaimers given so many papers in the wild:
 Assume we have access to a powerful pretrained language model.</description>
    </item>
    
    <item>
      <title>Generalized Language Models</title>
      <link>https://wuxb09.github.io/test-lilian/posts/2019-01-31-lm/</link>
      <pubDate>Thu, 31 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://wuxb09.github.io/test-lilian/posts/2019-01-31-lm/</guid>
      <description>[Updated on 2019-02-14: add ULMFiT and GPT-2.] [Updated on 2020-02-29: add ALBERT.] [Updated on 2020-10-25: add RoBERTa.] [Updated on 2020-12-13: add T5.] [Updated on 2020-12-30: add GPT-3.] [Updated on 2021-11-13: add XLNet, BART and ELECTRA; Also updated the Summary section.]
Fig. 0. I guess they are Elmo &amp; Bert? (Image source: here) We have seen amazing progress in NLP in 2018. Large-scale pre-trained language modes like OpenAI GPT and BERT have achieved great performance on a variety of language tasks using generic model architectures.</description>
    </item>
    
    <item>
      <title>Learning Word Embedding</title>
      <link>https://wuxb09.github.io/test-lilian/posts/2017-10-15-word-embedding/</link>
      <pubDate>Sun, 15 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://wuxb09.github.io/test-lilian/posts/2017-10-15-word-embedding/</guid>
      <description>Human vocabulary comes in free text. In order to make a machine learning model understand and process the natural language, we need to transform the free-text words into numeric values. One of the simplest transformation approaches is to do a one-hot encoding in which each distinct word stands for one dimension of the resulting vector and a binary value indicates whether the word presents (1) or not (0).
However, one-hot encoding is impractical computationally when dealing with the entire vocabulary, as the representation demands hundreds of thousands of dimensions.</description>
    </item>
    
  </channel>
</rss>
