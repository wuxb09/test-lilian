<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>data-augmentation on Lil&#39;Log</title>
    <link>https://wuxb09.github.io/test-lilian/tags/data-augmentation/</link>
    <description>Recent content in data-augmentation on Lil&#39;Log</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 15 Apr 2022 15:10:30 -0700</lastBuildDate><atom:link href="https://wuxb09.github.io/test-lilian/tags/data-augmentation/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Learning with not Enough Data Part 3: Data Generation</title>
      <link>https://wuxb09.github.io/test-lilian/posts/2022-04-15-data-gen/</link>
      <pubDate>Fri, 15 Apr 2022 15:10:30 -0700</pubDate>
      
      <guid>https://wuxb09.github.io/test-lilian/posts/2022-04-15-data-gen/</guid>
      <description>Here comes the Part 3 on learning with not enough data (Previous: Part 1 and Part 2). Letâ€™s consider two approaches for generating synthetic data for training.
 Augmented data. Given a set of existing training samples, we can apply a variety of augmentation, distortion and transformation to derive new data points without losing the key attributes. We have covered a bunch of augmentation methods on text and images in a previous post on contrastive learning.</description>
    </item>
    
    <item>
      <title>Contrastive Representation Learning</title>
      <link>https://wuxb09.github.io/test-lilian/posts/2021-05-31-contrastive/</link>
      <pubDate>Mon, 31 May 2021 00:00:00 +0000</pubDate>
      
      <guid>https://wuxb09.github.io/test-lilian/posts/2021-05-31-contrastive/</guid>
      <description>The goal of contrastive representation learning is to learn such an embedding space in which similar sample pairs stay close to each other while dissimilar ones are far apart. Contrastive learning can be applied to both supervised and unsupervised settings. When working with unsupervised data, contrastive learning is one of the most powerful approaches in self-supervised learning.
Contrastive Training Objectives In early versions of loss functions for contrastive learning, only one positive and one negative sample are involved.</description>
    </item>
    
  </channel>
</rss>
