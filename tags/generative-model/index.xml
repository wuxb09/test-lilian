<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>generative-model on Lil&#39;Log</title>
    <link>https://wuxb09.github.io/test-lilian/tags/generative-model/</link>
    <description>Recent content in generative-model on Lil&#39;Log</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 11 Jul 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://wuxb09.github.io/test-lilian/tags/generative-model/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>What are Diffusion Models?</title>
      <link>https://wuxb09.github.io/test-lilian/posts/2021-07-11-diffusion-models/</link>
      <pubDate>Sun, 11 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>https://wuxb09.github.io/test-lilian/posts/2021-07-11-diffusion-models/</guid>
      <description>[Updated on 2021-09-19: Highly recommend this blog post on score-based generative modeling by Yang Song (author of several key papers in the references)]. [Updated on 2022-08-27: Added classifier-free guidance, GLIDE, unCLIP and Imagen. [Updated on 2022-08-31: Added latent diffusion model.
So far, I&amp;rsquo;ve written about three types of generative models, GAN, VAE, and Flow-based models. They have shown great success in generating high-quality samples, but each has some limitations of its own.</description>
    </item>
    
    <item>
      <title>Curriculum for Reinforcement Learning</title>
      <link>https://wuxb09.github.io/test-lilian/posts/2020-01-29-curriculum-rl/</link>
      <pubDate>Wed, 29 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://wuxb09.github.io/test-lilian/posts/2020-01-29-curriculum-rl/</guid>
      <description>[Updated on 2020-02-03: mentioning PCG in the &amp;ldquo;Task-Specific Curriculum&amp;rdquo; section. [Updated on 2020-02-04: Add a new &amp;ldquo;curriculum through distillation&amp;rdquo; section.
It sounds like an impossible task if we want to teach integral or derivative to a 3-year-old who does not even know basic arithmetics. That&amp;rsquo;s why education is important, as it provides a systematic way to break down complex knowledge and a nice curriculum for teaching concepts from simple to hard.</description>
    </item>
    
    <item>
      <title>Self-Supervised Representation Learning</title>
      <link>https://wuxb09.github.io/test-lilian/posts/2019-11-10-self-supervised/</link>
      <pubDate>Sun, 10 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://wuxb09.github.io/test-lilian/posts/2019-11-10-self-supervised/</guid>
      <description>[Updated on 2020-01-09: add a new section on Contrastive Predictive Coding].  [Updated on 2020-04-13: add a &amp;ldquo;Momentum Contrast&amp;rdquo; section on MoCo, SimCLR and CURL.]  [Updated on 2020-07-08: add a &amp;ldquo;Bisimulation&amp;rdquo; section on DeepMDP and DBC.]  [Updated on 2020-09-12: add MoCo V2 and BYOL in the &amp;ldquo;Momentum Contrast&amp;rdquo; section.]  [Updated on 2021-05-31: remove section on &amp;ldquo;Momentum Contrast&amp;rdquo; and add a pointer to a full post on &amp;ldquo;Contrastive Representation Learning&amp;rdquo;]</description>
    </item>
    
    <item>
      <title>Flow-based Deep Generative Models</title>
      <link>https://wuxb09.github.io/test-lilian/posts/2018-10-13-flow-models/</link>
      <pubDate>Sat, 13 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>https://wuxb09.github.io/test-lilian/posts/2018-10-13-flow-models/</guid>
      <description>So far, I&amp;rsquo;ve written about two types of generative models, GAN and VAE. Neither of them explicitly learns the probability density function of real data, $p(\mathbf{x})$ (where $\mathbf{x} \in \mathcal{D}$) &amp;mdash; because it is really hard! Taking the generative model with latent variables as an example, $p(\mathbf{x}) = \int p(\mathbf{x}\vert\mathbf{z})p(\mathbf{z})d\mathbf{z}$ can hardly be calculated as it is intractable to go through all possible values of the latent code $\mathbf{z}$.
Flow-based deep generative models conquer this hard problem with the help of normalizing flows, a powerful statistics tool for density estimation.</description>
    </item>
    
    <item>
      <title>From Autoencoder to Beta-VAE</title>
      <link>https://wuxb09.github.io/test-lilian/posts/2018-08-12-vae/</link>
      <pubDate>Sun, 12 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>https://wuxb09.github.io/test-lilian/posts/2018-08-12-vae/</guid>
      <description>[Updated on 2019-07-18: add a section on VQ-VAE &amp;amp; VQ-VAE-2.]  [Updated on 2019-07-26: add a section on TD-VAE.] 
Autocoder is invented to reconstruct high-dimensional data using a neural network model with a narrow bottleneck layer in the middle (oops, this is probably not true for Variational Autoencoder, and we will investigate it in details in later sections). A nice byproduct is dimension reduction: the bottleneck layer captures a compressed latent encoding.</description>
    </item>
    
    <item>
      <title>From GAN to WGAN</title>
      <link>https://wuxb09.github.io/test-lilian/posts/2017-08-20-gan/</link>
      <pubDate>Sun, 20 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://wuxb09.github.io/test-lilian/posts/2017-08-20-gan/</guid>
      <description>[Updated on 2018-09-30: thanks to Yoonju, we have this post translated in Korean!]  [Updated on 2019-04-18: this post is also available on arXiv.]
Generative adversarial network (GAN) has shown great results in many generative tasks to replicate the real-world rich content such as images, human language, and music. It is inspired by game theory: two models, a generator and a critic, are competing with each other while making each other stronger at the same time.</description>
    </item>
    
  </channel>
</rss>
